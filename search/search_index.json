{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep Learning Algorithms","text":""},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project. (Not useful)</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"loss_functions/cross_entropy/","title":"Cross Entropy","text":""},{"location":"loss_functions/cross_entropy/#introduction","title":"Introduction","text":"<p>This blog will cover a very simple exercise, coding Cross entropy loss from scratch such that the loss value for a random <code>(input,target)</code> pair from our implementation is the same as the loss from PyTorch\u2019s cross Entropy implementation</p> <p></p>"},{"location":"loss_functions/cross_entropy/#pseudo-code","title":"Pseudo code","text":"<ul> <li>We will have a input logits of shape (Batch, number of classes) . For our sample we have a batch size of 3 and 5 classes.</li> <li>The targets will be integers of shape (batch, 1) . They will be integers between [0, num_classes -1]</li> <li>logits can be negative as well and in any scale ! not necessary between 0 and 1</li> <li>Step 1: We have to apply softmax for each row ( for each sample in a batch)</li> <li>Step 2: Choose the likelihood corresponding to target class and apply   <code>-log(probability_of_correct_class)</code> on it</li> <li>Step 3: take the mean of all log probabilities in a batch</li> </ul>"},{"location":"loss_functions/cross_entropy/#avoid-integer-overflow","title":"Avoid Integer overflow !","text":"<p>Note that to calculate softmax we have to take the exponent of the logits. For logits as big as 1000, the computer doesn\u2019t have enough bits to store this large number! To avoid this we do a neat trick</p> <p>In simple Words, subtracting a constant from the logits , won\u2019t change their softmax probability</p> <p></p> <p></p> <p>As you can see, the e^c terms cancel out in both numerator and denominator, proving that subtracting any constant (in our case, the maximum value) from all logits gives us the same softmax probabilities, but in a numerically stable way.</p> <p>The key step is that e^{x-c} = e^x/e^c, which lets the common factor e^c cancel out in the final division.</p>"},{"location":"loss_functions/cross_entropy/#code","title":"Code","text":"<pre><code>import torch\nimport numpy as np\n\ndef cross_entropy_loss(predictions, targets):\n    \"\"\"\n    Custom implementation of cross entropy loss\n\n    Args:\n        predictions: Raw model output logits of shape (batch_size, num_classes)\n        targets: Ground truth labels of shape (batch_size,)\n\n    Returns:\n        loss: Mean cross entropy loss across the batch\n    \"\"\"\n    # Get batch size\n    batch_size = predictions.shape[0]\n\n    # Apply softmax to get probabilities\n    exp_preds = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n    softmax_preds = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)\n\n    # Get predicted probability for the correct class\n    correct_class_probs = softmax_preds[range(batch_size), targets]\n\n    # Calculate negative log likelihood\n    loss = -np.log(correct_class_probs + 1e-7)  # Add small epsilon for numerical stability\n\n    # Return mean loss\n    return np.mean(loss)\n\n# Test the implementation\nif __name__ == \"__main__\":\n    # Generate sample data\n    np.random.seed(42)\n    batch_size = 3\n    num_classes = 4\n\n    # Create random logits and targets\n    logits = np.random.randn(batch_size, num_classes)\n    targets = np.random.randint(0, num_classes, size=batch_size)\n\n    # Calculate loss using our implementation\n    custom_loss = cross_entropy_loss(logits, targets)\n\n    # Calculate loss using PyTorch\n    torch_logits = torch.FloatTensor(logits)\n    torch_targets = torch.LongTensor(targets)\n    torch_loss = torch.nn.CrossEntropyLoss()(torch_logits, torch_targets)\n\n    print(\"Input logits:\")\n    print(logits)\n    print(\"\\nTarget labels:\", targets)\n    print(\"\\nCustom implementation loss:\", custom_loss)\n    print(\"PyTorch implementation loss:\", torch_loss.item())\n    print(\"\\nDifference:\", abs(custom_loss - torch_loss.item()))\n</code></pre>"},{"location":"loss_functions/kl_divergence/","title":"KL Divergence Loss","text":""}]}